# version: "3.8"

# services:
#   namenode:
#     image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
#     container_name: namenode
#     restart: always
#     ports:
#       - 9870:9870
#       - 9000:9000
#     volumes:
#       - hadoop_namenode:/hadoop/dfs/name
#     environment:
#       CLUSTER_NAME: hadoop-cluster
#       CORE_CONF_fs_defaultFS: hdfs://namenode:9000
#       HDFS_CONF_dfs_replication: 3

#   datanode1:
#     image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
#     container_name: datanode1
#     restart: always
#     depends_on:
#       - namenode
#     environment:
#       CORE_CONF_fs_defaultFS: hdfs://namenode:9000
#       HDFS_CONF_dfs_datanode_data_dir: file:///hadoop/dfs/data
#     volumes:
#       - hadoop_datanode1:/hadoop/dfs/data

#   datanode2:
#     image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
#     container_name: datanode2
#     restart: always
#     depends_on:
#       - namenode
#     environment:
#       CORE_CONF_fs_defaultFS: hdfs://namenode:9000
#       HDFS_CONF_dfs_datanode_data_dir: file:///hadoop/dfs/data
#     volumes:
#       - hadoop_datanode2:/hadoop/dfs/data

#   datanode3:
#     image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
#     container_name: datanode3
#     restart: always
#     depends_on:
#       - namenode
#     environment:
#       CORE_CONF_fs_defaultFS: hdfs://namenode:9000
#       HDFS_CONF_dfs_datanode_data_dir: file:///hadoop/dfs/data
#     volumes:
#       - hadoop_datanode3:/hadoop/dfs/data

#   datanode4:
#     image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
#     container_name: datanode4
#     restart: always
#     depends_on:
#       - namenode
#     environment:
#       CORE_CONF_fs_defaultFS: hdfs://namenode:9000
#       HDFS_CONF_dfs_datanode_data_dir: file:///hadoop/dfs/data
#     volumes:
#       - hadoop_datanode4:/hadoop/dfs/data

#   datanode5:
#     image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
#     container_name: datanode5
#     restart: always
#     depends_on:
#       - namenode
#     environment:
#       CORE_CONF_fs_defaultFS: hdfs://namenode:9000
#       HDFS_CONF_dfs_datanode_data_dir: file:///hadoop/dfs/data
#     volumes:
#       - hadoop_datanode5:/hadoop/dfs/data

#   resourcemanager:
#     image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
#     container_name: resourcemanager
#     restart: always
#     depends_on:
#       - namenode
#     ports:
#       - 8088:8088
#     environment:
#       CORE_CONF_fs_defaultFS: hdfs://namenode:9000
#       YARN_CONF_yarn_resourcemanager_hostname: resourcemanager
#       YARN_CONF_yarn_nodemanager_aux_services: mapreduce_shuffle
#       YARN_CONF_yarn_resourcemanager_resource_tracker_address: resourcemanager:8031
#       YARN_CONF_yarn_resourcemanager_scheduler_address: resourcemanager:8030
#       YARN_CONF_yarn_resourcemanager_address: resourcemanager:8032
#       YARN_CONF_yarn_resourcemanager_admin_address: resourcemanager:8033

#   nodemanager:
#     image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
#     container_name: nodemanager
#     restart: always
#     depends_on:
#       - resourcemanager
#     environment:
#       CORE_CONF_fs_defaultFS: hdfs://namenode:9000
#       YARN_CONF_yarn_resourcemanager_hostname: resourcemanager
#       YARN_CONF_yarn_nodemanager_aux_services: mapreduce_shuffle

#   historyserver:
#     image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
#     container_name: historyserver
#     restart: always
#     depends_on:
#       - namenode
#       - resourcemanager
#       - nodemanager
#     environment:
#       CORE_CONF_fs_defaultFS: hdfs://namenode:9000
#       MAPRED_CONF_mapreduce_jobhistory_address: historyserver:10020
#       MAPRED_CONF_mapreduce_jobhistory_webapp_address: historyserver:19888
#       YARN_CONF_yarn_resourcemanager_hostname: resourcemanager
#     ports:
#       - 19888:19888

#   hive-server:
#     image: bde2020/hive:2.3.2-postgresql-metastore
#     container_name: hive-server
#     # volumes:
#     #   - ./employee:/employee
#     env_file:
#       - ./hadoop-hive.env
#     environment:
#       HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
#       #SERVICE_PRECONDITION: "hive-metastore:9083"
#     depends_on:
#       - hive-metastore
#     ports:
#       - "10000:10000"

#   hive-metastore:
#     image: bde2020/hive:2.3.2-postgresql-metastore
#     container_name: hive-metastore
#     env_file:
#       - ./hadoop-hive.env
#     command: /opt/hive/bin/hive --service metastore
#     # environment:
#     #   SERVICE_PRECONDITION: "namenode:50070 datanode:50075 hive-metastore-postgresql:5432"
#     depends_on:
#       - hive-metastore-postgresql
#     ports:
#       - "9083:9083"

#   hive-metastore-postgresql:
#     image: bde2020/hive-metastore-postgresql:2.3.0
#     container_name: hive-metastore-postgresql
#     volumes:
#       - ./metastore-postgresql/postgresql/data:/var/lib/postgresql/data
#     depends_on:
#       - namenode
  
  
#   spark:
#     image: bitnami/spark:latest
#     container_name: spark
#     user: root
#     depends_on:
#       - namenode
#       - resourcemanager
#       - nodemanager
#       - hive-server
#     environment:
#       SPARK_MODE: master
#       SPARK_WORKER_MEMORY: 1G
#       SPARK_WORKER_CORES: 1
#       SPARK_LOG_LEVEL: INFO
#       HADOOP_CONF_DIR: /opt/hadoop/etc/hadoop
#       YARN_CONF_DIR: /opt/hadoop/etc/hadoop
#       IVY_HOME: /ivy2
#       SPARK_LOCAL_DIRS: /tmp/spark-local
#       SPARK_SUBMIT_OPTS: "-Dspark.jars.ivy=/ivy2 -Dspark.local.dir=/tmp/spark-local -Divy.cache.dir=/ivy2 -Divy.local.dir=/ivy2"
#       PATH: /opt/bitnami/python/bin:/opt/bitnami/java/bin:/opt/bitnami/spark/bin:/opt/bitnami/spark/sbin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
#     ports:
#       - "8080:8080"
#       - "7077:7077"
#     volumes:
#       - ./hadoop-config:/opt/hadoop/etc/hadoop
#       - ./ml_task.py:/opt/spark/ml_task.py
#     entrypoint: ["sh", "-c", "apt-get update && apt-get install -y python3-pip && pip3 install pandas scikit-learn pyspark && python3 /opt/spark/ml_task.py && tail -f /dev/null"]

# volumes:
#   hadoop_namenode:
#   hadoop_datanode1:
#   hadoop_datanode2:
#   hadoop_datanode3:
#   hadoop_datanode4:
#   hadoop_datanode5:
#   #hive-metastore-postgresql-data:
#   hive-metastore-mysql-data:


version: "3.8"

services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - 9870:9870
      - 9000:9000
      - 8020:8020
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      CLUSTER_NAME: hadoop-cluster
      CORE_CONF_fs_defaultFS: hdfs://namenode:8020
      HDFS_CONF_dfs_replication: 1
    networks:
      - hadoop

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    depends_on:
      - namenode
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:8020
      HDFS_CONF_dfs_datanode_data_dir: file:///hadoop/dfs/data
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    networks:
      - hadoop

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    restart: always
    depends_on:
      - namenode
    ports:
      - 8088:8088
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:8020
      YARN_CONF_yarn_resourcemanager_hostname: resourcemanager
      YARN_CONF_yarn_nodemanager_aux_services: mapreduce_shuffle
      YARN_CONF_yarn_resourcemanager_resource_tracker_address: resourcemanager:8031
      YARN_CONF_yarn_resourcemanager_scheduler_address: resourcemanager:8030
      YARN_CONF_yarn_resourcemanager_address: resourcemanager:8032
      YARN_CONF_yarn_resourcemanager_admin_address: resourcemanager:8033
    networks:
      - hadoop

  nodemanager:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager
    restart: always
    depends_on:
      - resourcemanager
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:8020
      YARN_CONF_yarn_resourcemanager_hostname: resourcemanager
      YARN_CONF_yarn_nodemanager_aux_services: mapreduce_shuffle
    networks:
      - hadoop

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    restart: always
    depends_on:
      - namenode
      - resourcemanager
      - nodemanager
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:8020
      MAPRED_CONF_mapreduce_jobhistory_address: historyserver:10020
      MAPRED_CONF_mapreduce_jobhistory_webapp_address: historyserver:19888
      YARN_CONF_yarn_resourcemanager_hostname: resourcemanager
    ports:
      - 19888:19888
    networks:
      - hadoop

  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0
    container_name: hive-metastore-postgresql
    volumes:
      - ./metastore-postgresql/postgresql/data:/var/lib/postgresql/data
    networks:
      - hadoop

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    env_file:
      - ./hadoop-hive.env
    command: /opt/hive/bin/hive --service metastore
    depends_on:
      - hive-metastore-postgresql
    ports:
      - "9083:9083"
    networks:
      - hadoop

  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    env_file:
      - ./hadoop-hive.env
    environment:
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
    depends_on:
      - hive-metastore
    ports:
      - "10000:10000"
    networks:
      - hadoop

  spark:
    image: bitnami/spark:latest
    container_name: spark
    user: root
    depends_on:
      - namenode
      - resourcemanager
      - nodemanager
      - hive-server
    environment:
      SPARK_MODE: master
      SPARK_WORKER_MEMORY: 1G
      SPARK_WORKER_CORES: 1
      SPARK_LOG_LEVEL: INFO
      HADOOP_CONF_DIR: /opt/hadoop/etc/hadoop
      YARN_CONF_DIR: /opt/hadoop/etc/hadoop
      IVY_HOME: /ivy2
      SPARK_LOCAL_DIRS: /tmp/spark-local
      SPARK_SUBMIT_OPTS: "-Dspark.jars.ivy=/ivy2 -Dspark.local.dir=/tmp/spark-local -Divy.cache.dir=/ivy2 -Divy.local.dir=/ivy2"
      PATH: /opt/bitnami/python/bin:/opt/bitnami/java/bin:/opt/bitnami/spark/bin:/opt/bitnami/spark/sbin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./hadoop-config:/opt/hadoop/etc/hadoop
      - ./ml_task.py:/opt/spark/ml_task.py
    entrypoint: ["sh", "-c", "apt-get update && apt-get install -y python3-pip && pip3 install pandas matplotlib scikit-learn pyspark && python3 /opt/spark/ml_task.py && tail -f /dev/null"]
    networks:
      - hadoop

networks:
  hadoop:
    driver: bridge

volumes:
  hadoop_namenode:
  hadoop_datanode:
